{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Queries with and without Azure OpenAI"
      ],
      "metadata": {},
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you have your Search Engine loaded **from two different data sources in two diferent text-based indexes**, on this notebook we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results.\n",
        "\n",
        "The idea is that a user can ask a question about Computer Science (first datasource/index) or about Covid (second datasource/index), and the engine will respond accordingly.\n",
        "This **Multi-Index** demo, mimics the scenario where a company loads multiple type of documents of different types and about completly different topics and the search engine must respond with the most relevant results."
      ],
      "metadata": {},
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up variables"
      ],
      "metadata": {},
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.embeddings import AzureOpenAIEmbeddings\n",
        "\n",
        "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_PROMPT_TEMPLATE\n",
        "from common.utils import (\n",
        "    get_search_results,\n",
        "    model_tokens_limit,\n",
        "    num_tokens_from_docs,\n",
        "    num_tokens_from_string\n",
        ")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1707001329453
        }
      },
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f"
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ[\"AZURE_OPENAI_ENDPOINT\"]) #YK to check if it uses correct values"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "https://openaiykus.openai.azure.com/\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707001332587
        }
      },
      "id": "c9d1dd3e-0ae3-4169-ad2d-fe76c92b2688"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1707001337590
        }
      },
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Index Search queries"
      ],
      "metadata": {},
      "id": "9297d29b-1f61-4dce-858e-bf4272172dba"
    },
    {
      "cell_type": "code",
      "source": [
        "# Text-based Indexes that we are going to query (from Notebook 01 and 02)\n",
        "index1_name = \"cogsrch-index-files-ykilinc\" #YK\n",
        "index2_name = \"cogsrch-index-csv-ykilinc\"#YK\n",
        "indexes = [index1_name, index2_name]"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "gather": {
          "logged": 1707001530498
        }
      },
      "id": "5a46e2d3-298a-4708-83de-9e108b1a117a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try questions that you think might be answered or addressed in computer science papers in 2020-2021 or that can be addressed by medical publications about COVID in 2020-2021. Try comparing the results with the open version of ChatGPT.<br>\n",
        "The idea is that the answers using Azure OpenAI only looks at the information contained on these publications.\n",
        "\n",
        "**Example Questions you can ask**:\n",
        "- What is CLP?\n",
        "- How Markov chains work?\n",
        "- What are some examples of reinforcement learning?\n",
        "- What are the main risk factors for Covid-19?\n",
        "- What medicine reduces inflamation in the lungs?\n",
        "- Why Covid doesn't affect kids that much compared to adults?\n",
        "- Does chloroquine really works against covid?\n",
        "- Who won the 1994 soccer world cup? # This question should yield no answer if the system is correctly grounded"
      ],
      "metadata": {},
      "id": "1c62ebb2-d7be-4bfb-b1ba-4db86c11839a"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Tell me about large stable models problem?\""
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1707003315289
        }
      },
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search on both indexes individually and aggragate results\n",
        "\n",
        "#### **Note**: \n",
        "In order to standarize the indexes, **there must be 6 mandatory fields present on each text-based index**: `id, title, chunks, name, location, vectorized`. This is so that each document can be treated the same along the code. Also, **all indexes must have a semantic configuration**."
      ],
      "metadata": {},
      "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf"
    },
    {
      "cell_type": "code",
      "source": [
        "agg_search_results = dict()\n",
        "# search operation on multiple Azure Search indexes. \n",
        "for index in indexes:\n",
        "    search_payload = {\n",
        "        \"search\": QUESTION,\n",
        "        \"select\": \"id, title, chunks, name, location\", #fields\n",
        "        \"queryType\": \"semantic\",\n",
        "        \"semanticConfiguration\": \"my-semantic-config\",\n",
        "        \"count\": \"true\",\n",
        "        \"speller\": \"lexicon\",\n",
        "        \"queryLanguage\": \"en-us\",\n",
        "        \"captions\": \"extractive\",\n",
        "        \"answers\": \"extractive\",\n",
        "        \"top\": \"10\"\n",
        "    }\n",
        "\n",
        "    r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index + \"/docs/search\",\n",
        "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
        "    print(r.status_code)\n",
        "\n",
        "    search_results = r.json()\n",
        "    agg_search_results[index]=search_results\n",
        "    print(\"Index:\", index, \"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "200\nIndex: cogsrch-index-files-ykilinc Results Found: 6, Results Returned: 6\n200\nIndex: cogsrch-index-csv-ykilinc Results Found: 18, Results Returned: 10\n"
        }
      ],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1707003320407
        }
      },
      "id": "faf2e30f-e71f-4533-ab52-27d048b80a89"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the top results (from both searches) based on the score"
      ],
      "metadata": {
        "tags": []
      },
      "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389"
    },
    {
      "cell_type": "code",
      "source": [
        "# agg_search_results\n",
        "# if you would like to see the results from AI Search"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707003396528
        }
      },
      "id": "1902104f-42dd-4f87-b951-48874de74aea"
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML('<h4>Top Answers</h4>'))\n",
        "\n",
        "for index,search_results in agg_search_results.items():#iterates over the dict\n",
        "    for result in search_results['@search.answers']:\n",
        "        if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
        "            display(HTML('<h5>' + 'Answer - score: ' + str(round(result['score'],2)) + '</h5>'))\n",
        "            display(HTML(result['text']))\n",
        "            \n",
        "print(\"\\n\\n\")\n",
        "display(HTML('<h4>Top Results</h4>'))\n",
        "\n",
        "content = dict()\n",
        "ordered_content = OrderedDict()\n",
        "\n",
        "\n",
        "for index,search_results in agg_search_results.items():\n",
        "    for result in search_results['value']:\n",
        "        if result['@search.rerankerScore'] > 1:# Show answers that are at least 25% of the max possible score=4\n",
        "            content[result['id']]={\n",
        "                                    \"title\": result['title'],\n",
        "                                    \"chunks\": result['chunks'], \n",
        "                                    \"chunks_vectors\": [],\n",
        "                                    \"name\": result['name'], \n",
        "                                    \"location\": result['location'] ,\n",
        "                                    \"caption\": result['@search.captions'][0]['text'],\n",
        "                                    \"score\": result['@search.rerankerScore'],\n",
        "                                    \"index\": index\n",
        "                                    }\n",
        "    \n",
        "#After results have been filtered we will Sort and add them as an Ordered list\\n\",\n",
        "for id in sorted(content, key= lambda x: content[x][\"score\"], reverse=True):\n",
        "    ordered_content[id] = content[id]\n",
        "    url = str(ordered_content[id]['location']) + os.environ['BLOB_SAS_TOKEN']\n",
        "    title = str(ordered_content[id]['title']) if (ordered_content[id]['title']) else ordered_content[id]['name']\n",
        "    score = str(round(ordered_content[id]['score'],2))\n",
        "    display(HTML('<h5><a href=\"'+ url + '\">' + title + '</a> - score: '+ score + '</h5>'))\n",
        "    display(HTML(ordered_content[id]['caption']))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Top Answers</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n\n\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h4>Top Results</h4>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">arXiv:cs/0002001v1  [cs.LO]  3 Feb 2000</a> - score: 2.89</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Specifically, we study the following two problems (|P | stands for the number of rules in a logic program P ):  LSM (Large stable models) Given a finite propositional logic program P and an integer k, decide whether there is a stable model of P of size at least |P | − k."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0001007v11.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">Microsoft Word - Effect of different packet sizes on RED performance.doc</a> - score: 1.4</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "This problem is less important when the propagation delay is relatively high (see Figure 4). We can see from Figure 3 and Figure 5 that in all cases TCP SACK experiences a higher PLR compared to TCP Reno."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">Application of the ARIMA model on the COVID-2019 epidemic dataset</a> - score: 1.36</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Coronavirus disease 2019 (COVID-2019) has been recognized as a global threat, and several studies are being conducted using various mathematical models to predict the probable evolution of this epidemic. These mathematical models based on various factors and analyses are subject to potential bias."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">A conceptual model for the outbreak of Coronavirus disease 2019 (COVID-19) in Wuhan, China with individual reaction and governmental action</a> - score: 1.3</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "A conceptual model for the outbreak of Coronavirus disease 2019 (COVID-19) in Wuhan, China with individual reaction and governmental action. The ongoing Coronavirus Disease 2019 (COVID-19) outbreak, originated in the end of 2019 in Wuhan, China, has claimed more than 2200 lives and posed a huge threat to global public health."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/arxiv_cs_pdf_0001_0001001v11.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">arXiv:cs/0001001v1  [cs.OH]  3 Jan 2000</a> - score: 1.25</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Here n and N = 2n are  integer numbers. 1) x : dot = [1 .. N, 1 .. N]; Here x is represented as point on 2D space  for simplicity, because of digitization we can con- sider space of any dimension as interval of natural numbers x : 1 .. N2. In this example x is visual model for n + n = 2n-bits register."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">An updated estimation of the risk of transmission of the novel coronavirus (2019-nCov)</a> - score: 1.16</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "We estimated when the effective daily reproduction ratio has fallen below 1 and when the epidemics will peak. Our updated findings suggest that the best measure is persistent and strict self-isolation."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">COVID-19 outbreak on the Diamond Princess cruise ship: estimating the epidemic potential and effectiveness of public health countermeasures</a> - score: 1.13</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "We showed that an early evacuation of all passengers on 3 February would have been associated with 76 infected persons in their incubation time.The cruise ship conditions clearly amplified an already highly transmissible disease. The public health measures prevented more than 2000 additional cases compared to no interventions."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">Optimization Method for Forecasting Confirmed Cases of COVID-19 in China</a> - score: 1.09</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Optimization Method for Forecasting Confirmed Cases of COVID-19 in China. In December 2019, a novel coronavirus, called COVID-19, was discovered in Wuhan, China, and has spread to different cities in China as well as to 24 other countries. The number of confirmed cases is increasing daily and reached 34,598 on 8 February 2020."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<h5><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-02-29T05:08:30Z&st=2024-01-31T21:08:30Z&spr=https&sig=XjAfUkgiU%2F9xZT4iHdA9SYfeZT1geBPxV2ELzdaO42Y%3D\">Real-time forecasts of the 2019-nCoV epidemic in China from February 5th to February 24th, 2020</a> - score: 1.07</h5>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Our most recent forecasts reported here based on data up until February 9, 2020, largely agree across the three models presented and suggest an average range of 7,409 – 7,496 additional cases in Hubei and 1,128 – 1,929 additional cases in other provinces within the next five days."
          },
          "metadata": {}
        }
      ],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1707003325446
        }
      },
      "id": "9e938337-602d-4b61-8141-b8c92a5d91da"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments on Query results"
      ],
      "metadata": {},
      "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above the semantic search feature of Azure Cognitive Search service is good. It gives answers (sometimes) and also the top results with the corresponding file and the paragraph where the answers is possible located.\n",
        "\n",
        "Let's see if we can make this better with Azure OpenAI"
      ],
      "metadata": {},
      "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Azure OpenAI\n",
        "\n",
        "To use OpenAI to get a better answer to our question, the thought process is simple: let's **give the answer and the content of the documents from the search result to the GPT model as context and let it provide a better response**.\n",
        "\n",
        "Now, before we do this, we need to understand a few things first:\n",
        "\n",
        "1) Chainning and Prompt Engineering\n",
        "2) Embeddings\n",
        "\n",
        "We will use a library call **LangChain** that wraps a lot of boiler plate code.\n",
        "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
      ],
      "metadata": {},
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1707003485455
        }
      },
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note**: Starting now, we will utilize OpenAI models. Please ensure that you have deployed the following models within the Azure OpenAI portal using these precise deployment names:\n",
        "\n",
        "- text-embedding-ada-002\n",
        "- gpt-35-turbo\n",
        "- gpt-35-turbo-16k\n",
        "- gpt-4\n",
        "- gpt-4-32k\n",
        "\n",
        "Should you have deployed the models under different names, the code provided below will not function as expected. To resolve this, you would need to modify the variable names throughout all the notebooks."
      ],
      "metadata": {},
      "id": "325d9138-2250-4f6b-bc88-50d7957f8d33"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A gentle intro to chaining LLMs and prompt engineering"
      ],
      "metadata": {},
      "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains are what you get by connecting one or more large language models (LLMs) in a logical way. (Chains can be built of entities other than LLMs but for now, let’s stick with this definition for simplicity).\n",
        "\n",
        "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
        "\n",
        "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
        "\n",
        "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
        "\n",
        "Here’s an example:"
      ],
      "metadata": {},
      "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4-32k\" # options: gpt-35-turbo, gpt-35-turbo-16k, gpt-4, gpt-4-32k\n",
        "COMPLETION_TOKENS = 800\n",
        "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=COMPLETION_TOKENS)"
      ],
      "outputs": [],
      "execution_count": 67,
      "metadata": {
        "gather": {
          "logged": 1707004935387
        }
      },
      "id": "13df9247-e784-4e04-9475-55e672efea47"
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"language\"],\n",
        "    template='Answer the following question: \"{question}\". Give your response in {language}',\n",
        ")\n",
        "\n",
        "print(prompt.format(question=QUESTION, language=\"French\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Answer the following question: \"Tell me about large stable models problem?\". Give your response in French\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1707003536460
        }
      },
      "id": "7b0520b9-83b2-49fd-ad84-624cb0f15ce1"
    },
    {
      "cell_type": "code",
      "source": [
        "# And finaly we create our first generic chain\n",
        "chain_chat = LLMChain(llm=llm, prompt=prompt)\n",
        "chain_chat({\"question\": QUESTION, \"language\": \"French\"})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": "{'question': 'Tell me about large stable models problem?',\n 'language': 'French',\n 'text': \"Le problème des grands modèles stables fait référence à un défi dans le domaine de l'intelligence artificielle et de l'apprentissage automatique. Il s'agit de la difficulté à créer et à maintenir des modèles stables qui peuvent gérer de grandes quantités de données. Ces modèles doivent être capables de traiter et d'analyser efficacement ces données sans compromettre leur performance ou leur précision. C'est un problème particulièrement pertinent dans les domaines où de grandes quantités de données sont utilisées, comme la reconnaissance d'images ou le traitement du langage naturel. La création de modèles stables capables de gérer ces grandes quantités de données est un domaine de recherche actif dans le domaine de l'intelligence artificielle.\"}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1707003553447
        }
      },
      "id": "dcc7dae3-6b88-4ea6-be43-b178ebc559dc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: this is the first time you use OpenAI in this Accelerator, so if you get a Resource not found error, is most likely because the name of your OpenAI model deployment is different than the variable MODEL set above"
      ],
      "metadata": {},
      "id": "cd8539d0-a538-4368-82c3-5f91d8370f1e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
        "\n",
        "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
      ],
      "metadata": {},
      "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The second type of Chains are Utility:**\n",
        "\n",
        "* Utility — These are specialized chains, comprised of many LLMs to help solve a specific task. For example, LangChain supports some end-to-end chains (such as QA_WITH_SOURCES for QnA Doc retrieval, Summarization, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). \n",
        "\n",
        "We will look at one specific chain called **qa_with_sources** in this workshop for digging deeper and solve our use case of enhancing the results of Azure Cognitive Search."
      ],
      "metadata": {
        "tags": []
      },
      "id": "12c48038-b1af-4228-8ffb-720e554fd3b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "But before dealing with the utility chain needed, we need to deal first with this problem: \n",
        "\n",
        "**the content of the search result files is or can be very lengthy, more than the allowed tokens allowed by the GPT Azure OpenAI models**. \n",
        "\n",
        "This is where the concept of embeddings/vectors come into place.\n",
        "\n",
        "## Embeddings and Vector Search\n",
        "\n",
        "From the Azure OpenAI documentation ([HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=python)), An **embedding** is a special format of data representation that can be easily utilized by machine learning models and algorithms. <u>**The embedding is an information dense representation of the semantic meaning of a piece of text**</u>. Each embedding is a vector of floating point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar. \n",
        "\n",
        "To address the challenge of accommodating context within the token limit of a Language Model (LLM), the solution involves the following steps:\n",
        "\n",
        "1. **Segmenting Documents**: Divide the documents into smaller segments or chunks.\n",
        "2. **Vectorization of Chunks**: Transform these chunks into vectors using appropriate techniques.\n",
        "3. **Vector Semantic Search**: Execute a semantic search using vectors to identify the top chunks similar to the given question.\n",
        "4. **Optimal Context Provision**: Provide the LLM with the most relevant and concise context, thereby achieving an optimal balance between comprehensiveness and lengthiness.\n",
        "\n",
        "\n",
        "Notice that **the documents chunks are already done in Azure Search**. *ordered_content* dictionary (created a few cells above) contains the chunks of each document. So we don't really need to chunk them again, but we still need to make sure that we can be as fast as possible and that we are below the max allowed input token limits of our selected OpenAI model."
      ],
      "metadata": {},
      "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our ultimate goal is to rely solely on vector indexes. While it is possible to manually code parsers with OCR for various file types and develop a scheduler to synchronize data with the index, there is a more efficient alternative: **Azure Cognitive Search is soon going to release automated chunking strategies and vectorization within the next months**, so we have three options: \n",
        "1. Wait for this functionality while in the meantime manually push chunks and its vectors to the vector-based indexes \n",
        "2. Fill up the vector-based indexes on-demand, as documents are discovered by users\n",
        "3. Use custom skills (for chunking and vectorization) and use knowledge stores in order to create a vector-base index from a text-based-ai-enriched index at ingestion time. See [HERE](https://github.com/Azure/cognitive-search-vector-pr/blob/main/demo-python/code/azure-search-vector-ingestion-python-sample.ipynb) for instructions on how to do this.\n",
        "\n",
        "In this notebook we are going to implement Option 2: **Create vector-based indexes per each text-based indexes and fill them up on-demand as documents are discovered**. Why? because is simpler and quick to implement, while we wait for Option 1 to become a 'feature of Azure Search Engine' (which is the automation of Option 3 inside the search engine).\n",
        "\n",
        "As observed in Notebooks 1 and 2, each text-based index contains a field named `vectorized` that we have not utilized yet. We will now harness this field. **The objective is to avoid vectorizing all documents at the time of ingestion** (Option 3). Instead, we can **vectorize the chunks as users search for or discover documents.** This approach ensures that we allocate funds and resources only when the documents are actually required. Typically, in an organization with a vast repository of documents in a data lake, only 20% of the documents are frequently accessed, while the rest remain untouched. This phenomenon mirrors the [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle) found in nature."
      ],
      "metadata": {},
      "id": "80e79235-3d8b-4713-9336-5004cc4a1556"
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"cogsrch-index-files-ykilinc\" #YK\n",
        "index2_name = \"cogsrch-index-csv-ykilinc\" #YK\n",
        "indexes = [index_name, index2_name]"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1707003788452
        }
      },
      "id": "12682a1b-df92-49ce-a638-7277103f6cb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not duplicate code, we have put many of the code used above into functions. These functions are in the `common/utils.py` and `common/prompts.py` files. This way we can use these functios in the app that we will build later."
      ],
      "metadata": {},
      "id": "78a6d6a7-18ef-45b2-a216-3c1f50006593"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10 # Number of results per each text_index\n",
        "ordered_results = get_search_results(QUESTION, indexes, k, reranker_threshold=1)\n",
        "print(\"Number of results:\",len(ordered_results))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of results: 9\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1707004092296
        }
      },
      "id": "3bccca45-d1dd-476f-b109-a528b857b6b3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the below line if you want to inspect the ordered results\n",
        "# ordered_results"
      ],
      "outputs": [],
      "execution_count": 60,
      "metadata": {
        "gather": {
          "logged": 1707004161387
        }
      },
      "id": "7714f38a-daaa-4fc5-a95a-dd025d153216"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can **fill up the vector-based index **as users lookup documents using the text-based index. This approach although it requires **two searches per user query (one on the text-based indexes and the other one on the vector-based indexes)**, it is simpler to implement and will be incrementatly faster as user use the system."
      ],
      "metadata": {},
      "id": "da70e7a8-7536-4688-b30c-01ba28e9b9f8"
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\", skip_empty=True) "
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1706786848362
        }
      },
      "id": "2937ba3b-098d-43f8-8498-3534882a5cc7"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for key,value in ordered_results.items():\n",
        "    if value[\"vectorized\"] != True: # If the document has not been vectorized yet\n",
        "        i = 0\n",
        "        print(\"Vectorizing\",len(value[\"chunks\"]),\"chunks from Document:\",value[\"location\"])\n",
        "        for chunk in value[\"chunks\"]: # Iterate over the document's text chunks\n",
        "            try:\n",
        "                upload_payload = {  # Insert the chunk and its vector in the vector-based index\n",
        "                    \"value\": [\n",
        "                        {\n",
        "                            \"id\": key + \"_\" + str(i),\n",
        "                            \"title\": f\"{value['title']}_chunk_{str(i)}\",\n",
        "                            \"chunk\": chunk,\n",
        "                            \"chunkVector\": embedder.embed_query(chunk if chunk!=\"\" else \"-------\"),\n",
        "                            \"name\": value[\"name\"],\n",
        "                            \"location\": value[\"location\"],\n",
        "                            \"@search.action\": \"upload\"\n",
        "                        },\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "                r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + value[\"index\"]+\"-vector\" + \"/docs/index\",\n",
        "                                     data=json.dumps(upload_payload), headers=headers, params=params)\n",
        "                \n",
        "                if r.status_code != 200:\n",
        "                    print(r.status_code)\n",
        "                    print(r.text)\n",
        "                else:\n",
        "                    i = i + 1 # increment chunk number\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(\"Exception:\",e)\n",
        "                print(chunk)\n",
        "                continue\n",
        "                    \n",
        "        # Update document in text-based index and mark it as \"vectorized\"\n",
        "        upload_payload = {\n",
        "            \"value\": [\n",
        "                {\n",
        "                    \"id\": key,\n",
        "                    \"vectorized\": True,\n",
        "                    \"@search.action\": \"merge\"\n",
        "                },\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + value[\"index\"]+ \"/docs/index\",\n",
        "                         data=json.dumps(upload_payload), headers=headers, params=params)\n",
        "                    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Vectorizing 11 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf\nVectorizing 5 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0001007v11.pdf\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nVectorizing 3 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/arxiv_cs_pdf_0001_0001001v11.pdf\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nVectorizing 1 chunks from Document: https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv\nCPU times: user 1.94 s, sys: 1.83 ms, total: 1.95 s\nWall time: 21.1 s\n"
        }
      ],
      "execution_count": 61,
      "metadata": {},
      "id": "f664df30-99c3-4a30-8cb0-42ba3044e5b0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: **How the text-based and the vector-based indexes stay in sync?**\n",
        "For **document changes**, the problem is already taken care of, since Azure Engine will update the text-based index automatically if a file has a new version. This puts the **vectorized field in None **and the next time that the file is searched it will be vectorized again into the vector-based index.\n",
        "\n",
        "However for **deletion of files**, the problem is half solved. Azure Search engine **would delete the documents in the text-based index** if the file is deleted on the source, however you will need to **code a script that runs on a fixed schedule that looks for deleted ids in the text-based index and deletes the corresponding chunks in the vector-based index.**"
      ],
      "metadata": {},
      "id": "f490b7fe-eec2-4c96-a2f2-f8ab0a1b2098"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we search on the vector-based indexes and get the top k most similar chunks to our question:"
      ],
      "metadata": {},
      "id": "1f67f3a2-0023-4f5a-b52f-3fb071cfd8e1"
    },
    {
      "cell_type": "code",
      "source": [
        "vector_indexes = [index+\"-vector\" for index in indexes]\n",
        "\n",
        "k = 10\n",
        "top_k = 4  \n",
        "ordered_results = get_search_results(QUESTION, vector_indexes , #!!!!!!\n",
        "                                        k=k, # Number of results per vector index\n",
        "                                        reranker_threshold=1,\n",
        "                                        vector_search=True, #!!\n",
        "                                        similarity_k=top_k,# # After results have been filtered, sort and add the top k to the ordered_contentr\n",
        "                                        query_vector = embedder.embed_query(QUESTION)#!!!\n",
        "                                        )\n",
        "print(\"Number of results:\",len(ordered_results))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of results: 4\n"
        }
      ],
      "execution_count": 64,
      "metadata": {
        "gather": {
          "logged": 1707004761327
        }
      },
      "id": "61098bb4-33da-4eb4-94cf-503587337aca"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For vector search is not recommended to give more than k=5 chunks (of max 5000 characters each) to the LLM as context. Otherwise you can have issues later with the token limit trying to have a conversation with memory."
      ],
      "metadata": {},
      "id": "1a98a974-0633-499f-a8f0-29bf6242e737"
    },
    {
      "cell_type": "code",
      "source": [
        "top_docs = []\n",
        "for key,value in ordered_results.items():\n",
        "    location = value[\"location\"] if value[\"location\"] is not None else \"\"# only location is added without sas token. So we can't reach the file at the end\n",
        "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location})) \n",
        "        \n",
        "print(\"Number of chunks:\",len(top_docs))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of chunks: 4\n"
        }
      ],
      "execution_count": 65,
      "metadata": {
        "gather": {
          "logged": 1707004837476
        }
      },
      "id": "7dfb9e39-2542-469d-8f64-4c0c26d79535"
    },
    {
      "cell_type": "code",
      "source": [
        "#top_docs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706787891055
        }
      },
      "id": "2aab3243-53fc-4356-9890-2ea5573a0620"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate number of tokens of our docs\n",
        "if(len(top_docs)>0):\n",
        "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
        "    prompt_tokens = num_tokens_from_string(COMBINE_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
        "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
        "    \n",
        "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
        "    \n",
        "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
        "    \n",
        "    print(\"System prompt token count:\",prompt_tokens)\n",
        "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
        "    print(\"Combined docs (context) token count:\",context_tokens)\n",
        "    print(\"--------\")\n",
        "    print(\"Requested token count:\",requested_tokens)\n",
        "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
        "    print(\"Chain Type selected:\", chain_type)\n",
        "        \n",
        "else:\n",
        "    print(\"NO RESULTS FROM AZURE SEARCH\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "System prompt token count: 1669\nMax Completion Token count: 800\nCombined docs (context) token count: 3700\n--------\nRequested token count: 6169\nToken limit for gpt-4-32k : 32768\nChain Type selected: stuff\n"
        }
      ],
      "execution_count": 68,
      "metadata": {
        "gather": {
          "logged": 1707004956429
        }
      },
      "id": "880885fe-16bd-44bb-9556-7cb3d4989993"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use our Utility Chain from LangChain `qa_with_sources`"
      ],
      "metadata": {},
      "id": "1e232424-c7ba-4153-b23b-fb1fa2ebc64b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A central question for building a **summarizer is how to pass your documents into the LLM’s context window**. Two common approaches for this are:\n",
        "\n",
        "**Stuff**: Simply “stuff” all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\n",
        "\n",
        "**Map-reduce**: Summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\n",
        "\n",
        "Map step applies LLM to each document and classifies the topics. \n",
        "\n",
        "Reduce Step combines the topics generated from all documents.\n",
        "\n",
        "LangChain’s MapReduce approach allows us to tackle large text corpora beyond the single prompt token limit, making it a powerful tool for text analysis! "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "59ea2884-8194-410e-b756-3fa40973e74c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Chain Types](./images/langchain.png)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d60258b1-373c-471b-b582-4300b2255953"
    },
    {
      "cell_type": "code",
      "source": [
        "if chain_type == \"stuff\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       prompt=COMBINE_PROMPT)\n",
        "elif chain_type == \"map_reduce\":\n",
        "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
        "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
        "                                       combine_prompt=COMBINE_PROMPT,\n",
        "                                       return_intermediate_steps=True)"
      ],
      "outputs": [],
      "execution_count": 69,
      "metadata": {
        "gather": {
          "logged": 1707006241350
        }
      },
      "id": "511273b3-256d-4e60-be72-ccd4a74cb885"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Try with other language as well\n",
        "response = chain({\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"English\"})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 24.4 ms, sys: 0 ns, total: 24.4 ms\nWall time: 13.1 s\n"
        }
      ],
      "execution_count": 70,
      "metadata": {},
      "id": "b99a0c19-d48c-41e9-8d6c-6d9f13d29da3"
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 71,
          "data": {
            "text/plain": "{'input_documents': [Document(page_content='\\nar\\nX\\n\\niv\\n:c\\n\\ns/\\n00\\n\\n02\\n00\\n\\n1v\\n1 \\n\\n [\\ncs\\n\\n.L\\nO\\n\\n] \\n 3\\n\\n F\\neb\\n\\n 2\\n00\\n\\n0\\n\\nComputing large and small stable models1\\n\\nMiros law Truszczyński\\n\\nUniversity of Kentucky\\n\\nLexington, KY 40506-0046, USA\\n\\nmirek@cs.uky.edu\\n\\nAbstract\\n\\nIn this paper, we focus on the problem of existence and computing of small and large stable\\nmodels. We show that for every fixed integer k, there is a linear-time algorithm to decide\\nthe problem LSM (large stable models problem): does a logic program P have a stable\\nmodel of size at least |P | − k. In contrast, we show that the problem SSM (small stable\\nmodels problem) to decide whether a logic program P has a stable model of size at most\\nk is much harder. We present two algorithms for this problem but their running time is\\ngiven by polynomials of order depending on k. We show that the problem SSM is fixed-\\nparameter intractable by demonstrating that it is W [2]-hard. This result implies that it is\\nunlikely, an algorithm exists to compute stable models of size at most k that would run\\nin time O(nc), where c is a constant independent of k. We also provide an upper bound\\non the fixed-parameter complexity of the problem SSM by showing that it belongs to the\\nclass W [3].\\n\\n1 Introduction\\n\\nThe stable model semantics by Gelfond and Lifschitz [10] is one of the two most widely\\nstudied semantics for normal logic programs, the other one being the well-founded seman-\\ntics by Van Gelder, Ross and Schlipf [17]. Among 2-valued semantics, the stable model\\nsemantics is commonly regarded as the one providing the correct meaning to the negation\\noperator in logic programming. It coincides with the least model semantics on the class\\nof Horn programs, and with the well-founded semantics and the perfect model semantics\\non the class of stratified programs [1]. In addition, the stable model semantics is closely\\nrelated to the notion of a default extension by Reiter [12, 4]. Logic programming with sta-\\nble model semantics has applications in knowledge representation, planning and reasoning\\nabout action. It was also recently proposed as a computational paradigm especially well\\nsuited for solving combinatorial optimization and constraint satisfaction problems [14, 15].\\n\\nThe problem with the stable model semantics is that, even in the propositional case,\\nreasoning with logic programs under the stable model semantics is computationally hard. It\\nis well-known that deciding whether a finite propositional logic program has a stable model\\nis NP-complete [13]. Consequently, it is not at all clear that logic programming with the\\nstable model semantics can serve as a practical computational tool.\\n\\n1This is a full version of an extended abstract presented at the International Conference on Logic Pro-\\ngramming, ICLP-99 and included in the proceedings published by MIT Press.\\n\\n1\\n\\nhttp://arxiv.org/abs/cs/0002001v1\\n\\n\\nThis issue can be resolved by implementing systems computing stable models and by\\nexperimentally studying the performance of these systems. Several such projects are now\\nunder way. Niemelä and Simons [16] developed a system, smodels, for computing stable\\nmodels of finite function symbol-free logic programs and reported very promising perfor-\\nmance results. For some classes of programs, smodels decides the existence of a stable\\nmodel in a matter of seconds even if an input program consists of tens of thousands of\\nclauses. Encouraging results on using smodels to solve planning problems are reported\\nin [15]. Another well-advanced system is DeReS [6], designed to compute extensions of\\narbitrary propositional default theories but being especially effective for default theories\\nencoding propositional logic programs. Finally, systems capable of reasoning with disjunc-\\ntive logic programs were described in [9] and [2].\\n\\nHowever, faster implementations will ultimately depend on better understanding of the\\nalgorithmic aspects of reasoning with logic programs under the stable model semantics. In\\nthis paper, we investigate the complexity of deciding whether a finite propositional logic\\nprogram has stable models of some restricted sizes. Specifically, we study the following two\\nproblems (|P | stands for the number of rules in a logic program P ):\\n\\nLSM (Large stable models) Given a finite propositional logic program P and an integer k,\\ndecide whether there is a stable model of P of size at least |P | − k.\\n\\nSSM (Small stable models) Given a finite propositional logic program P and an integer k,\\ndecide whether there is a stable model of P of size no more than k.\\n\\nInputs to the problems LSM and SSM are pairs (P, k), where P is a finite propositional\\nlogic program and k is a non-negative integer. Problems of this type are referred to as\\nparametrized decision problems. By fixing a parameter, a parameterized decision problem\\ngives rise to its fixed-parameter version. In the case of problems LSM and SSM , by fixing k\\nwe obtain the following two fixed-parameter problems (k is now no longer a part of input):\\n\\n', metadata={'source': 'https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf'}),\n  Document(page_content='Coronavirus disease 2019 (COVID-2019) has been recognized as a global threat, and several studies are being conducted using various mathematical models to predict the probable evolution of this epidemic. These mathematical models based on various factors and analyses are subject to potential bias. Here, we propose a simple econometric model that could be useful to predict the spread of COVID-2019. We performed Auto Regressive Integrated Moving Average (ARIMA) model prediction on the Johns Hopkins epidemiological data to predict the epidemiological trend of the prevalence and incidence of COVID-2019. For further comparison or for future perspective, case definition and data collection have to be maintained in real time.', metadata={'source': 'https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv'}),\n  Document(page_content='The initial cluster of severe pneumonia cases that triggered the 2019-nCoV epidemic was identified in Wuhan, China in December 2019. While early cases of the disease were linked to a wet market, human-to-human transmission has driven the rapid spread of the virus throughout China. The Chinese government has implemented containment strategies of city-wide lockdowns, screening at airports and train stations, and isolation of suspected patients; however, the cumulative case count keeps growing every day. The ongoing outbreak presents a challenge for modelers, as limited data are available on the early growth trajectory, and the epidemiological characteristics of the novel coronavirus are yet to be fully elucidated. We use phenomenological models that have been validated during previous outbreaks to generate and assess short-term forecasts of the cumulative number of confirmed reported cases in Hubei province, the epicenter of the epidemic, and for the overall trajectory in China, excluding the province of Hubei. We collect daily reported cumulative case data for the 2019-nCoV outbreak for each Chinese province from the National Health Commission of China. Here, we provide 5, 10, and 15 day forecasts for five consecutive days, February 5th through February 9th, with quantified uncertainty based on a generalized logistic growth model, the Richards growth model, and a sub-epidemic wave model. Our most recent forecasts reported here based on data up until February 9, 2020, largely agree across the three models presented and suggest an average range of 7,409 – 7,496 additional cases in Hubei and 1,128 – 1,929 additional cases in other provinces within the next five days. Models also predict an average total cumulative case count between 37,415 – 38,028 in Hubei and 11,588 – 13,499 in other provinces by February 24, 2020. Mean estimates and uncertainty bounds for both Hubei and other provinces have remained relatively stable in the last three reporting dates (February 7th – 9th). We also observe that each of the models predicts that the epidemic has reached saturation in both Hubei and other provinces. Our findings suggest that the containment strategies implemented in China are successfully reducing transmission and that the epidemic growth has slowed in recent days.', metadata={'source': 'https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/csv/all_sources_metadata.csv'}),\n  Document(page_content='We will now modify the theory T0(P ) to construct a theory T (P ) that demonstrates\\nthat the problem SSM(k) can be reduced to the problem WS(3). First, for each atom\\nq ∈ At(P ), introduce k2 + 2k new atoms d(q, i), 1 ≤ i ≤ k2 + 2k, and define\\n\\nC0(q) = {¬c(q) ∨ d(q, i): 1 ≤ i ≤ k2 + 2k} ∪ {c(q) ∨ ¬d(q, i): 1 ≤ i ≤ k2 + 2k}.\\n\\nNext, define\\n\\nC1(q, i) = {¬c−(q, i) ∨ c(q, 1) ∨ . . . ∨ c(q, i− 1)} ∪ {¬c(q, j) ∨ c−(q, i): 1 ≤ j ≤ i− 1},\\n\\nC2(q) = {¬c(q) ∨ c(q, 1) ∨ . . . ∨ c(q, k + 1)} ∪ {¬c(q, j) ∨ c(q): 1 ≤ j ≤ k + 1},\\n\\nand\\n\\nC4(q, i) = {¬c(q, i) ∨ F3(r1, i) ∨ . . . ∨ F3(rt, i)} ∪ {¬F3(rj , i) ∨ c(q, i): 1 ≤ j ≤ m},\\n\\nwhere {r1, . . . , rt} is the set of all rules in P with q in the head.\\nFinally, define\\n\\nT (P ) = {C0(q): q ∈ At(P )} ∪ {C1(q, i): q ∈ At(P ), 2 ≤ i ≤ k + 1} ∪\\n\\n{C2(q): q ∈ At(P )} ∪ {C4(q, i): q ∈ At(P ), 1 ≤ i ≤ k + 1}\\n\\nIt is easy to see that the set of clauses C1(q, i) is equivalent to the formula F1(q, i), the set\\nof clauses C2(q) is equivalent to the formula F2(q), and the set C4(q, i) of disjunctions of\\nconjunctions of literals is equivalent to the formula F4(q, i). Thus, the union of the last three\\nsets of clauses in the definition of T (P ) is logically equivalent to the theory T0(P ). It follows\\nthat U ⊆ {c(q): q ∈ At} ∪ {c(q, i): q ∈M, 1 ≤ i ≤ k + 1} ∪ {c−(q, i): q ∈M, 2 ≤ i ≤ k + 1}\\nis a model of T0(P ) if and only if U ∪{d(q, i): c(q) ∈ U, 1 ≤ i ≤ k2 +2k} is a model of T (P ).\\nMoreover, every model of T (P ) is of the form U ∪{d(q, i): c(q) ∈ U, 1 ≤ i ≤ k2 +2k}, where\\nU ⊆ {c(q): q ∈ At} ∪ {c(q, i): q ∈ M, 1 ≤ i ≤ k + 1} ∪ {c−(q, i): q ∈ M, 2 ≤ i ≤ k + 1} is a\\nmodel of T0(P ).\\n\\nThe role of the clauses in the sets C0(q) is to decrease the effect of the atoms c−(q, i)\\nand c(q, i) on the size of models of T (P ). Consequently, given a model U of T0(P ), we\\ncan derive a bound on |{q ∈ At(P ): c(q) ∈ U}| from a bound on the size of the model of\\nT (P ) corresponding to U . Specifically, it is easy to see that T0(P ) has a model U with\\n|{q ∈ At(P ): c(q) ∈ U}| ≤ k if and only if T (P ) has a model of size at most (k +1)(k2 +2k).\\nThus, by Theorem 5.5, one can show that P has a stable model of size at most k if and only\\nif T (P ) has a model of size at most (k + 1)(k2 + 2k). In other words, the problem SSM\\ncan be reduced to the problem WS(3).\\n\\nTheorem 5.6 The problem SSM(k) ∈W [3].\\n\\nNext, we will show that the problem WS(2) can be reduced to the problem SSM . Let\\nC = {c1, . . . , cm} be a collection of clauses. Let A = {x1, . . . , xn} be the set of atoms\\nappearing in clauses in C. For each atom x ∈ A, introduce k new atoms x(i), 1 ≤ i ≤ k.\\nBy Si, 1 ≤ i ≤ k, we denote the logic program consisting of the following n clauses:\\n\\n14\\n\\n\\n\\nx1(i)← not(x2(i)), . . . ,not(xn(i))\\n· · ·\\n\\nxn(i)← not(x1(i)), . . . ,not(xn−1(i))\\n\\nDefine S =\\n⋃k\\n\\ni=1 Si. Clearly, each stable model of S is of the form {xj1(1), . . . , xjk\\n(k)},\\n\\nwhere 1 ≤ jp ≤ n for p = 1, . . . , k. Sets of this form can be viewed as representations of\\nnonempty subsets of the set A that have no more than k elements. This representation is\\nnot one-to-one, that is, some subsets have multiple representations.\\n\\nNext, define P1 to be the program consisting of the clauses\\n\\nxj ← xj(i), j = 1, . . . , n, i = 1, 2, . . . , k.\\n\\nStable models of the program S ∪ P1 are of the form {xj1(1), . . . , xjk\\n(k)} ∪M , where M\\n\\nis a nonempty subset of A such that |M | ≤ k and xj1, . . . , xjk\\nenumerate (possibly with\\n\\nrepetitions) all elements of M .\\nFinally, for each clause\\n\\nc = a1 ∨ . . . ∨ as ∨ ¬b1 ∨ . . . ∨ ¬bt\\n\\nfrom C define a logic program clause p(c):\\n\\np(c) = f ← b1, . . . , bt,not(a1), . . . ,not(as),not(f)\\n\\nwhere f is yet another new atom. Define P2 = {p(c): c ∈ C} and PC = S ∪ P1 ∪ P2.\\n\\nTheorem 5.7 A set of clauses C has a nonempty model with no more than k elements if\\nand only if the program PC has a stable model with no more than 2k elements.\\n\\nProof: Let M be a nonempty model of C such that |M | ≤ k. Let xj1, . . . , xjk\\nbe an enumera-\\n\\ntion of all elements of M (possibly with repetitions). Then the set M ′ = {xj1(1), . . . , xjk\\n(k)}∪\\n\\nM is a stable model of the program S ∪ P1. Since M is a model of C, it follows that\\nPC\\n\\nM ′ = (S ∪ P1)M ′ ∪ F , where F consists of the clauses of the form\\n\\nf ← b1, . . . , bt,\\n\\nsuch that t ≥ 1 and for some j, 1 ≤ j ≤ t, bj /∈M ′. Since M ′ = LM((S ∪P1)M ′), it follows\\nthat\\n\\nM ′ = LM((S ∪ P1)M ′ ∪ F ) = LM(PC\\nM ′).\\n\\nThus, M ′ is a stable model of PC . Since |M ′| ≤ 2k, the “only if” part of the assertion\\nfollows.\\n\\nConversely, assume that M ′ is a stable model of PC . Clearly, f /∈M ′. Consequently,\\n\\nLM((S ∪ P1)M ′) = LM((S ∪ P1 ∪ P2)M ′) = LM(PC\\nM ′) = M ′.\\n\\nThat is, M ′ is a stable model of S ∪ P1. As mentioned earlier, it follows that M ′ =\\n{xj1(1), . . . , xjk\\n\\n(k)} ∪M , where M is a nonempty subset of At(P ) such that |M | ≤ k and\\nxj1 , . . . , xjk\\n\\nis an enumeration of all elements of M .\\n\\n15\\n\\n\\n\\nConsider a clause c = a1 ∨ . . . ∨ as ∨ ¬b1 ∨ . . . ∨ ¬bt from C. Since M ′ is a stable model\\nof PC , it is a model of PC . ', metadata={'source': 'https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf'})],\n 'question': 'Tell me about large stable models problem?',\n 'language': 'English',\n 'output_text': 'The Large Stable Models (LSM) problem is a computational problem that focuses on the existence and computation of large stable models. It is defined as follows: given a finite propositional logic program P and an integer k, decide whether there is a stable model of P of size at least |P| - k. The LSM problem can be decided by a linear-time algorithm for every fixed integer k<sup><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf\" target=\"_blank\">[1]</a></sup>.\\n\\nOn the other hand, the Small Stable Models (SSM) problem is to decide whether a logic program P has a stable model of size at most k. This problem is much harder and the running time of algorithms for this problem is given by polynomials of order depending on k. The SSM problem is fixed-parameter intractable and is W[2]-hard, implying that it is unlikely that an algorithm exists to compute stable models of size at most k that would run in time O(nc), where c is a constant independent of k<sup><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf\" target=\"_blank\">[1]</a></sup>.'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 71,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1707006273403
        }
      },
      "id": "4cdd79c0-a9b3-4760-9fd1-59adc2db5b55"
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['output_text'])) # links can be seen in jupyter labs"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "The Large Stable Models (LSM) problem is a computational problem that focuses on the existence and computation of large stable models. It is defined as follows: given a finite propositional logic program P and an integer k, decide whether there is a stable model of P of size at least |P| - k. The LSM problem can be decided by a linear-time algorithm for every fixed integer k<sup><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf\" target=\"_blank\">[1]</a></sup>.\n\nOn the other hand, the Small Stable Models (SSM) problem is to decide whether a logic program P has a stable model of size at most k. This problem is much harder and the running time of algorithms for this problem is given by polynomials of order depending on k. The SSM problem is fixed-parameter intractable and is W[2]-hard, implying that it is unlikely that an algorithm exists to compute stable models of size at most k that would run in time O(nc), where c is a constant independent of k<sup><a href=\"https://blobstoragejed5nzg3k2jp6.blob.core.windows.net/cord19/0002001v11.pdf\" target=\"_blank\">[1]</a></sup>."
          },
          "metadata": {}
        }
      ],
      "execution_count": 72,
      "metadata": {
        "gather": {
          "logged": 1707006280441
        }
      },
      "id": "37f7fa67-f67b-402e-89e3-266d5d6d21d8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please Note**: There are some instances where, despite the answer's high accuracy and quality, the references are not done according to the instructions provided in the COMBINE_PROMPT. This behavior is anticipated when dealing with GPT-3.5 models. We will provide a more detailed explanation of this phenomenon towards the conclusion of Notebook 5."
      ],
      "metadata": {},
      "id": "05e27c75-bfd9-4304-b2fd-c8e30bcc0558"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if you want to inspect the results from map_reduce chain type, each top similar chunk summary (k=4 by default)\n",
        "\n",
        "# if chain_type == \"map_reduce\":\n",
        "#     for step in response['intermediate_steps']:\n",
        "#         display(HTML(\"<b>Chunk Summary:</b> \" + step))"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {},
      "id": "11345374-6420-4b36-b061-795d2a804c85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
        "- Utilizing Azure Cognitive Search, we conduct a **multi-index text-based search that identifies the top documents from each index.**\n",
        "- Utilizing Azure Cognitive Search's vector search, **we extract the most relevant chunks of information.**\n",
        "- Subsequently, **Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.**\n",
        "- Best of two worlds!"
      ],
      "metadata": {},
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
      ],
      "metadata": {},
      "id": "fdc6e2fe-1c34-4952-99ad-14940f022379"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}